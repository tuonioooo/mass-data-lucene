# 分词器

## 什么是Tokenizer-分词

分词器的工作就是分解文本流成词\(tokens\).在这个文本中,每一个token都是这些字符的一个子序列.一个分析器\(analyzer\)必须知道它所配置的字段,但是tokenizer不需要,分词器\(tokenizer\)从一个字符流\(reader\)读取数据,生成一个Token对象\(TokenStream\)的序列.



　　输入流中的一些字符可能会被丢弃,如空格和一些分隔符;也可能会被添加或者替换,如别名映射和缩写.一个token包含多种元数据除了它的原始文本值,如字段中词\(token\)出现的位置.因为分词器从输入文本中发散之后生成词\(tokens\),你是不能假定token的文本和字段中出现的文本相同的.在原始的文本中很有可能超过一个的token拥有相同的位置或者关联相同的偏移量\(offset\).如果你使用token元数据做高亮时,请注意这一点儿.

